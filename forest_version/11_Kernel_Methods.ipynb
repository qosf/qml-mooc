{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel methods are widespread in machine learning and they were particularly common before deep learning became a dominant paradigm. The core idea is to introduce a new notion of distance between high-dimensional data points by replacing the inner product $(x_i, x_j)$ by a function that retains many properties of the inner product, yet which is nonlinear. This function $k(.,.)$ is called a kernel. Then, in many cases, wherever a learning algorithm would use an inner product, the kernel function is used instead.\n",
    "\n",
    "The intuition is that the kernel function acts as an inner product on a higher dimensional space and encompasses some $\\phi(.)$ mapping from the original space of the data points to this space. So intuitively, the kernel function is $k(x_i, x_j)=(\\phi(x_i), \\phi(x_j))$. The hope is that points that were not linearly separable in the original space become linearly separable in the higher dimensional space. The $\\phi(.)$ function may map to an infinite dimensional space and it does not actually have to be specified. As long as the kernel function is positive semidefinite, the idea works.\n",
    "\n",
    "Many kernel-based learning algorithms are instance-based, which means that the final model retains some or all of the training instances and they play a role in the actual prediction. Support vector machines belong here: support vectors are the training instances which are critically important in defining the boundary between two classes. Some important kernels are listed below.\n",
    "\n",
    "| Name | &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Kernel function|\n",
    "|------|-----------------|\n",
    "|Linear | $(x_i,x_j)$|\n",
    "|Polynomial| $((x_i,x_j)+c)^d$|\n",
    "|Radial basis function|$\\exp(-\\gamma\\|x_i-x_j\\|^2)$|\n",
    "\n",
    "The choice of kernel and the parameters of the kernel are often arbitrary and either some trial and error on the dataset or hyperparameter optimization helps choose the right combination. Quantum computers naturally give rise to certain kernels and it is worth looking at a specific variant of how it is constructed.\n",
    "\n",
    "\n",
    "# Thinking backward: learning methods based on what the hardware can do\n",
    "\n",
    "Instead of twisting a machine learning algorithm until it only contains subroutines that have quantum variants, we can reverse our thinking and ask: given a piece of quantum hardware and its constraints, can we come up with a new learning method? For instance, interference is a very natural thing to do: we showed an option in the first notebook on quantum states, and it can also be done with a Hadamard gate. For instance, imagine that you have training vectors encoded in some register, and this register is entangled with the $|0\\rangle$ in the superposition of an ancilla. The ancilla's $|1\\rangle$ of the superposition is entangled with another register that contains the test vector. Applying the Hadamard on the ancilla interferes the test and training instances. Measuring and post-selecting on the ancilla gives rise to a kernel [[1](#1)].\n",
    "\n",
    "Let's get the basic initialization out of the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.272439Z",
     "start_time": "2019-02-01T23:28:07.249924Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyquil import Program, get_qc\n",
    "from pyquil.gates import *\n",
    "from forest_tools import *\n",
    "qvm_server, quilc_server, fc = init_qvm_and_quilc()\n",
    "\n",
    "qc = get_qc('4q-qvm', connection=fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are constructing an instance-based classifier: we will calculate a kernel between all training instances and a test example. In this sense, this learning algorithm is lazy: no actual learning happens and each prediction includes the entire training set.\n",
    "\n",
    "As a consequence, state preparation is critical to this protocol. We have to encode the training instances in a superposition in a register, and the test instances in another register. Consider the following training instances of the [Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris): $S = \\{(\\begin{bmatrix}0 \\\\ 1\\end{bmatrix}, 0), (\\begin{bmatrix}0.78861006 \\\\ 0.61489363\\end{bmatrix}, 1)\\}$, that is, one example from class 0 and one example from class 1. Furthermore, let's have two test instances, $\\{\\begin{bmatrix}-0.549\\\\ 0.836\\end{bmatrix}, \\begin{bmatrix}0.053 \\\\ 0.999\\end{bmatrix}\\}$. These examples were cherry-picked because they are relatively straightforward to prepare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.280094Z",
     "start_time": "2019-02-01T23:28:07.275078Z"
    }
   },
   "outputs": [],
   "source": [
    "training_set = [[0, 1], [0.78861006, 0.61489363]]\n",
    "labels = [0, 1]\n",
    "test_set = [[-0.549, 0.836], [0.053 , 0.999]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We use amplitude encoding, which means that, for instance, the second training vector will be encoded as $0.78861006|0\\rangle + 0.61489363|1\\rangle$. Preparing these vectors only needs a rotation, and we only need to specify the corresponding angles. The first element of the training set does not even need that: it is just the $|1\\rangle$ state, so we don't specify an angle for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.287147Z",
     "start_time": "2019-02-01T23:28:07.281871Z"
    }
   },
   "outputs": [],
   "source": [
    "test_angles = [4.30417579487669/2, 3.0357101997648965/2]\n",
    "training_angle = 1.3245021469658966/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function does the state preparation. We plot it and explain it in more details below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.308617Z",
     "start_time": "2019-02-01T23:28:07.289821Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_state(angles):\n",
    "    ancilla_qubit = 0\n",
    "    index_qubit = 1\n",
    "    data_qubit = 2\n",
    "    class_qubit = 3\n",
    "    circuit = Program()\n",
    "    \n",
    "    # Put the ancilla and the index qubits into uniform superposition\n",
    "    circuit += H(ancilla_qubit)\n",
    "    circuit += H(index_qubit)\n",
    "\n",
    "    # Prepare the test vector\n",
    "    circuit += CNOT(ancilla_qubit, data_qubit)\n",
    "    circuit += RZ(-angles[0], data_qubit)\n",
    "    circuit += CNOT(ancilla_qubit, data_qubit)\n",
    "    circuit += RZ(angles[0], data_qubit)\n",
    "    # Flip the ancilla qubit > this moves the input \n",
    "    # vector to the |0> state of the ancilla\n",
    "    circuit += X(ancilla_qubit)\n",
    "\n",
    "    # Prepare the first training vector\n",
    "    # [0,1] -> class 0\n",
    "    # We can prepare this with a Toffoli\n",
    "    circuit += CCNOT(ancilla_qubit, index_qubit, data_qubit)\n",
    "    # Flip the index qubit > moves the first training vector to the \n",
    "    # |0> state of the index qubit\n",
    "    circuit += X(index_qubit)\n",
    "\n",
    "    # Prepare the second training vector\n",
    "    # [0.78861, 0.61489] -> class 1\n",
    "\n",
    "    circuit += CCNOT(ancilla_qubit, index_qubit, data_qubit)\n",
    "    circuit += CNOT(index_qubit, data_qubit)\n",
    "    circuit += RZ(angles[1], data_qubit)\n",
    "    circuit += CNOT(index_qubit, data_qubit)\n",
    "    circuit += RZ(-angles[1], data_qubit)\n",
    "    circuit += CCNOT(ancilla_qubit, index_qubit, data_qubit)\n",
    "    circuit += CNOT(index_qubit, data_qubit)\n",
    "    circuit += RZ(-angles[1], data_qubit)\n",
    "    circuit += CNOT(index_qubit, data_qubit)\n",
    "    circuit += RZ(angles[1], data_qubit)\n",
    "\n",
    "    # Flip the class label for training vector #2\n",
    "    circuit += CNOT(index_qubit, class_qubit)\n",
    "\n",
    "    return circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see the circuit for preparing state with the first test instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.315328Z",
     "start_time": "2019-02-01T23:28:07.310136Z"
    }
   },
   "outputs": [],
   "source": [
    "# from qiskit.tools.visualization import circuit_drawer\n",
    "angles = [test_angles[0], training_angle]\n",
    "state_preparation_0 = prepare_state(angles)\n",
    "# circuit_drawer(state_preparation_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vertical lines are barriers to make sure that all gates are finished by that point. They also make a natural segmentation of the state preparation.\n",
    "\n",
    "The test instance is prepared until the first barrier. The ancilla and index qubits (registers 0 and 1) are put into the uniform superposition. The test instance is entangled with the ground state of the ancilla. \n",
    "\n",
    "Then between the first and second barriers, we prepare the state $|1\\rangle$, which is the first training instance, and entangle it with the excited state of the ancilla and the ground state of the index qubit with a Toffoli gate and a Pauli-X gate. The Toffoli gate is also called the controlled-controlled-not gate, describing its action.\n",
    "\n",
    "The third section prepares the second training instance and entangles it with the excited state of the ancilla and the index qubit.\n",
    "\n",
    "The final part flips the class qubit conditioned on the index qubit. This creates the connection between the encoded training instances and the corresponding class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A natural kernel on a shallow circuit\n",
    "\n",
    "Having down the state preparation, the actual prediction is nothing but a Hadamard gate applied on the ancilla, followed by measurements. Since the ancilla is in a uniform superposition at the end of the state preparation and it is entangled with the registers encoding the test and training instances, applying a second Hadamard on the ancilla interferes the entangled registers. The state before the measurement is $\\frac{1}{2\\sqrt{2}}\\sum_{i=0}^1|0\\rangle|i\\rangle(|x_t\\rangle+|x_i\\rangle)|y_i\\rangle+|1\\rangle|i\\rangle(|x_t\\rangle-|x_i\\rangle)|y_i\\rangle$, where $|x_t\\rangle$ is the encoded test instance and $|x_i\\rangle$ is a training instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.324344Z",
     "start_time": "2019-02-01T23:28:07.316871Z"
    }
   },
   "outputs": [],
   "source": [
    "def interfere_data_and_test_instances(circuit, angles):\n",
    "    ro = circuit.declare(name='ro', memory_type='BIT', memory_size=4)\n",
    "    circuit += H(0)\n",
    "    for q in range(4):\n",
    "        circuit += MEASURE(q, ro[q])\n",
    "        \n",
    "    return circuit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we measure the ancilla, the outcome probability of observing 0 will be $\\frac{1}{4N}\\sum_{i=1}^N |x_t + x_i|^2$. This creates a kernel of the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.451780Z",
     "start_time": "2019-02-01T23:28:07.325731Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "x = np.linspace(-2, 2, 100)\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(0, 1.1)\n",
    "plt.plot(x, 1-x**2/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the kernel that performs the classification. We perform the post-selection on observing 0 on the measurement on the ancilla and calculate the probabilities of the test instance belonging to either class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:07.461051Z",
     "start_time": "2019-02-01T23:28:07.453155Z"
    }
   },
   "outputs": [],
   "source": [
    "def postselect(result_counts):\n",
    "    total_samples = sum(result_counts.values())\n",
    "\n",
    "    # define lambda function that retrieves only results where the ancilla is in the |0> state\n",
    "    post_select = lambda counts: [(state, occurences) for state, occurences in counts.items() if state[-1] == '0']\n",
    "\n",
    "    # perform the postselection\n",
    "    postselection = dict(post_select(result_counts))\n",
    "    postselected_samples = sum(postselection.values())\n",
    "\n",
    "    print(f'Ancilla post-selection probability was found to be {postselected_samples/total_samples}')\n",
    "\n",
    "    retrieve_class = lambda binary_class: [occurences for state, occurences in postselection.items() if state[0] == str(binary_class)]\n",
    "\n",
    "    prob_class0 = sum(retrieve_class(0))/postselected_samples\n",
    "    prob_class1 = sum(retrieve_class(1))/postselected_samples\n",
    "\n",
    "    print('Probability for class 0 is', prob_class0)\n",
    "    print('Probability for class 1 is', prob_class1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first instance we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:08.884566Z",
     "start_time": "2019-02-01T23:28:07.462294Z"
    }
   },
   "outputs": [],
   "source": [
    "circuit_0 = interfere_data_and_test_instances(state_preparation_0, angles)\n",
    "circuit_0.wrap_in_numshots_loop(1024)\n",
    "executable = qc.compile(circuit_0)\n",
    "measures = qc.run(executable)\n",
    "\n",
    "count = np.unique(measures, return_counts=True, axis=0)\n",
    "count = dict(zip(list(map(lambda l: ''.join(list(map(str, l))), count[0].tolist())), count[1]))\n",
    "print(count)\n",
    "\n",
    "postselect(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the second one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:09.224684Z",
     "start_time": "2019-02-01T23:28:08.886624Z"
    }
   },
   "outputs": [],
   "source": [
    "angles = [test_angles[1], training_angle]\n",
    "state_preparation_1 = prepare_state(angles)\n",
    "\n",
    "circuit_1 = interfere_data_and_test_instances(state_preparation_1, angles)\n",
    "executable = qc.compile(circuit_1)\n",
    "measures = qc.run(executable)\n",
    "\n",
    "count = np.unique(measures, return_counts=True, axis=0)\n",
    "count = dict(zip(list(map(lambda l: ''.join(list(map(str, l))), count[0].tolist())), count[1]))\n",
    "print(count)\n",
    "\n",
    "postselect(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T23:28:09.229717Z",
     "start_time": "2019-02-01T23:28:09.226853Z"
    }
   },
   "outputs": [],
   "source": [
    "qvm_server.terminate()\n",
    "quilc_server.terminate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "[1] M. Schuld, M. Fingerhuth, F. Petruccione. (2017). [Implementing a distance-based classifier with a quantum interference circuit](https://doi.org/10.1209/0295-5075/119/60002). *Europhysics Letters*, 119(6), 60002. <a id='1'></a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
